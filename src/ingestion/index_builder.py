# src/ingestion/index_builder.py
import os
import threading
from typing import Optional, Dict
from llama_index.core import VectorStoreIndex, StorageContext, load_index_from_storage
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.core.storage.docstore import SimpleDocumentStore
from llama_index.core.storage.index_store import SimpleIndexStore
from llama_index.core.schema import TextNode, Document
from src.core.logger import log, error, warn
from config.settings import STORAGE_DIR


class _IndexCache:
    def __init__(self):
        self._cache: Dict[str, VectorStoreIndex] = {}
        self._lock = threading.RLock()

    def get(self, kb_name: str) -> Optional[VectorStoreIndex]:
        with self._lock:
            return self._cache.get(kb_name)

    def set(self, kb_name: str, index: VectorStoreIndex):
        with self._lock:
            self._cache[kb_name] = index

    def invalidate(self, kb_name: str):
        with self._lock:
            if kb_name in self._cache:
                del self._cache[kb_name]


_index_cache = _IndexCache()


def get_or_build_index(kb_name: str, chroma_client, use_cache: bool = True) -> VectorStoreIndex:
    # 1. ç¼“å­˜å±‚
    if use_cache:
        cached_index = _index_cache.get(kb_name)
        if cached_index is not None:
            return cached_index

    try:
        coll_name = f"kb_{kb_name}"
        collection = chroma_client.get_or_create_collection(coll_name)
        vector_store = ChromaVectorStore(chroma_collection=collection)

        # æŒä¹…åŒ–ç›®å½•
        kb_persist_dir = os.path.join(STORAGE_DIR, f"docstore_{kb_name}")
        os.makedirs(kb_persist_dir, exist_ok=True)

        vector_count = collection.count()

        # å°è¯•ä»ç£ç›˜æ¢å¤ StorageContext
        try:
            storage_context = StorageContext.from_defaults(
                vector_store=vector_store,
                persist_dir=kb_persist_dir
            )

            if vector_count > 0:
                log(f"ä»ç£ç›˜åŠ è½½å®Œæ•´ç´¢å¼•: {kb_name}")
                index = load_index_from_storage(storage_context, vector_store=vector_store)

                # âœ… éªŒè¯ DocStore å®Œæ•´æ€§
                if _validate_docstore(index, collection):
                    log(f"âœ… DocStore éªŒè¯é€šè¿‡: {kb_name}")
                else:
                    # DocStore ä¸å®Œæ•´ï¼Œéœ€è¦é‡å»º
                    warn(f"æ£€æµ‹åˆ° DocStore ä¸ ChromaDB ä¸ä¸€è‡´,æ­£åœ¨ä¿®å¤...")
                    index = _rebuild_docstore_from_chroma(vector_store, kb_persist_dir, collection)
            else:
                # Chroma ç©ºï¼Œåˆå§‹åŒ–ç©ºç´¢å¼•
                index = VectorStoreIndex.from_documents([], storage_context=storage_context)

        except Exception as e:
            warn(f"DocStore åŠ è½½å¤±è´¥ ({e})ï¼Œæ­£åœ¨é‡å»º...")

            if vector_count > 0:
                # ä» ChromaDB é‡å»º
                index = _rebuild_docstore_from_chroma(vector_store, kb_persist_dir, collection)
            else:
                # åˆå§‹åŒ–ç©ºç´¢å¼•
                docstore = SimpleDocumentStore()
                index_store = SimpleIndexStore()
                storage_context = StorageContext.from_defaults(
                    vector_store=vector_store,
                    docstore=docstore,
                    index_store=index_store
                )
                log(f"ğŸ†• åˆå§‹åŒ–ç©ºç´¢å¼•: {kb_name}")
                index = VectorStoreIndex.from_documents([], storage_context=storage_context)
                index.storage_context.persist(persist_dir=kb_persist_dir)

        # 3. ç¼“å­˜
        if use_cache:
            _index_cache.set(kb_name, index)
        return index

    except Exception as e:
        error(f"âŒ ç´¢å¼•æ„å»ºä¸¥é‡å¤±è´¥: {kb_name} - {e}")
        raise


def _validate_docstore(index: VectorStoreIndex, collection) -> bool:
    """éªŒè¯ DocStore æ˜¯å¦å®Œæ•´"""
    try:
        docstore = index.docstore
        chroma_count = collection.count()

        # è·å–å‡ ä¸ª ID æµ‹è¯•
        results = collection.get(limit=min(10, chroma_count), include=["metadatas"])
        test_ids = results.get("ids", [])

        missing_count = 0
        for node_id in test_ids:
            try:
                docstore.get_node(node_id)
            except:
                missing_count += 1

        if missing_count > 0:
            warn(f"DocStore ç¼ºå¤±ç‡è¿‡é«˜: {missing_count}/{len(test_ids)}")
            return False

        return True

    except Exception as e:
        warn(f"éªŒè¯ DocStore å¤±è´¥: {e}")
        return False


def _rebuild_docstore_from_chroma(
        vector_store: ChromaVectorStore,
        kb_persist_dir: str,
        collection
) -> VectorStoreIndex:
    """
    ä» ChromaDB é‡å»º DocStore
    æ­£ç¡®åˆ›å»º TextNodeï¼Œä¸ç›´æ¥è®¾ç½® ref_doc_id
    """
    log("ä» ChromaDB é‡å»º DocStore...")

    # åˆ›å»ºæ–°çš„å­˜å‚¨ç»„ä»¶
    docstore = SimpleDocumentStore()
    index_store = SimpleIndexStore()

    storage_context = StorageContext.from_defaults(
        vector_store=vector_store,
        docstore=docstore,
        index_store=index_store
    )

    try:
        # è·å–æ‰€æœ‰æ•°æ®
        results = collection.get(include=["documents", "metadatas", "embeddings"])

        node_count = len(results["ids"])
        log(f"ä» ChromaDB è·å–åˆ° {node_count} ä¸ªèŠ‚ç‚¹")

        success_count = 0

        # âœ… å…³é”®ä¿®å¤ï¼šæ­£ç¡®åˆ›å»ºèŠ‚ç‚¹
        for idx, node_id in enumerate(results["ids"]):
            try:
                text = results["documents"][idx]
                metadata = results["metadatas"][idx]

                # æ–¹æ¡ˆ1: å¦‚æœæœ‰ doc_idï¼Œåˆ›å»º Document
                doc_id = metadata.get("doc_id") or metadata.get("ref_doc_id")

                if doc_id:
                    # åˆ›å»º Documentï¼ˆä¼šè‡ªåŠ¨è®¾ç½® doc_idï¼‰
                    doc = Document(
                        text=text,
                        id_=doc_id,
                        metadata=metadata,
                        excluded_embed_metadata_keys=["file_name", "file_path"],
                        excluded_llm_metadata_keys=["file_name", "file_path"]
                    )
                    docstore.add_documents([doc])

                    # å†åˆ›å»ºå¯¹åº”çš„ TextNode
                    node = TextNode(
                        text=text,
                        id_=node_id,
                        metadata=metadata,
                        excluded_embed_metadata_keys=["file_name", "file_path"],
                        excluded_llm_metadata_keys=["file_name", "file_path"]
                    )
                    # âœ… é€šè¿‡ relationships å…³è” Document
                    from llama_index.core.schema import NodeRelationship, RelatedNodeInfo
                    node.relationships[NodeRelationship.SOURCE] = RelatedNodeInfo(
                        node_id=doc_id,
                        metadata={}
                    )
                    docstore.add_documents([node])
                else:
                    # å¦‚æœæ²¡æœ‰ doc_idï¼Œç›´æ¥åˆ›å»ºç‹¬ç«‹ TextNode
                    node = TextNode(
                        text=text,
                        id_=node_id,
                        metadata=metadata,
                        excluded_embed_metadata_keys=["file_name", "file_path"],
                        excluded_llm_metadata_keys=["file_name", "file_path"]
                    )
                    docstore.add_documents([node])

                success_count += 1

            except Exception as e:
                warn(f"é‡å»ºèŠ‚ç‚¹å¤±è´¥ {node_id}: {e}")
                continue

        log(f"âœ… DocStore é‡å»ºå®Œæˆ,æˆåŠŸ {success_count}/{node_count} ä¸ªèŠ‚ç‚¹")

        # éªŒè¯é‡å»ºç»“æœ
        if success_count == 0:
            error("âŒ DocStore éªŒè¯å¤±è´¥: æ²¡æœ‰èŠ‚ç‚¹è¢«æ­£ç¡®ä¿å­˜!")
            log(f"DocStore å†…å®¹: {len(docstore.docs)} ä¸ªæ–‡æ¡£")

        # æŒä¹…åŒ–
        log("ğŸ’¾ æ­£åœ¨æŒä¹…åŒ– DocStore...")
        storage_context.persist(persist_dir=kb_persist_dir)

        # éªŒè¯æ–‡ä»¶å¤§å°
        docstore_path = os.path.join(kb_persist_dir, "docstore.json")
        if os.path.exists(docstore_path):
            size = os.path.getsize(docstore_path)
            if size < 100:  # å°äº 100 å­—èŠ‚è¯´æ˜åŸºæœ¬æ˜¯ç©ºçš„
                warn(f"DocStore æ–‡ä»¶è¿‡å°: {size} bytes")

        log("å·²ä¿å­˜é‡å»ºçš„ DocStore")

        # ä»é‡å»ºçš„ storage_context åˆ›å»ºç´¢å¼•
        index = VectorStoreIndex.from_vector_store(
            vector_store=vector_store,
            storage_context=storage_context
        )

        return index

    except Exception as e:
        error(f"âŒ é‡å»º DocStore å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise


def invalidate_index_cache(kb_name: str):
    """æ¸…é™¤ç´¢å¼•ç¼“å­˜"""
    _index_cache.invalidate(kb_name)