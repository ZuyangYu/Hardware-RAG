# src/core/custom_rag_chat.py
from typing import List, Tuple, Generator
from llama_index.core import Settings
from llama_index.core.llms import ChatMessage, MessageRole
from src.core.hybrid_retriever import hybrid_retrieve
from src.core.logger import log, error
import hashlib
import re


class CustomRAGChat:
    """
    è‡ªå®šä¹‰ RAG èŠå¤©å®ç°
    """

    def __init__(self, kb_name: str, index):
        self.kb_name = kb_name
        self.index = index
        self._context_cache = {}  # ä¸Šä¸‹æ–‡ç¼“å­˜ï¼š{query_hash: context}
        self._cache_size_limit = 10  # æœ€å¤šç¼“å­˜ 10 æ¡

    def _get_query_hash(self, query: str) -> str:
        """ç”ŸæˆæŸ¥è¯¢çš„å“ˆå¸Œå€¼ï¼ˆç”¨äºç¼“å­˜ï¼‰"""
        return hashlib.md5(query.strip().lower().encode()).hexdigest()

    def retrieve_context(self, query: str, top_k: int = 5) -> Tuple[str, str]:
        """
        æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡ï¼ˆå¸¦ç¼“å­˜ï¼‰
        Returns:
            Tuple[str, str]: (ç”¨äºPromptçš„çº¯æ–‡æœ¬ä¸Šä¸‹æ–‡, ç”¨äºUIæ˜¾ç¤ºçš„å¸¦æ ¼å¼ä¸Šä¸‹æ–‡)
        """
        query_hash = self._get_query_hash(query)
        if query_hash in self._context_cache:
            log(f"âš¡ ä½¿ç”¨ç¼“å­˜çš„ä¸Šä¸‹æ–‡: {query[:30]}...")
            return self._context_cache[query_hash]

        retrieved_nodes = hybrid_retrieve(query, self.index, self.kb_name, top_k)
        if not retrieved_nodes:
            return "", ""

        context_parts, display_parts = [], []
        for i, node in enumerate(retrieved_nodes, 1):
            content = node.node.get_content().strip()
            file_name = node.node.metadata.get('file_name', 'æœªçŸ¥æ¥æº')
            score = node.score if node.score else 0.0
            context_parts.append(f"ã€æ¥æº: {file_name}ã€‘\n{content}")
            safe_content = content[:200].replace('\n', ' ')
            display_parts.append(f"ã€æ¥æº {i}: {file_name} | åˆ†æ•°: {score:.4f}ã€‘\n{safe_content}...")

        context, display_context = "\n\n".join(context_parts), "\n\n".join(display_parts)
        log("=" * 50)
        log(f"ğŸ” [RAG æ£€ç´¢è¯¦æƒ…] Query: {query}")
        log(f"ğŸ“„ æ£€ç´¢åˆ° {len(retrieved_nodes)} ä¸ªç‰‡æ®µ")
        log("=" * 50)

        result = (context, display_context)
        self._context_cache[query_hash] = result
        if len(self._context_cache) > self._cache_size_limit:
            oldest_key = next(iter(self._context_cache))
            del self._context_cache[oldest_key]
        return result

    def chat(self, user_input: str, history: List[Tuple[str, str]], max_history: int = 5) -> Generator[str, None, None]:
        """ä¸»èŠå¤©æ–¹æ³• - è¿”å›æµå¼å“åº”çš„ç”Ÿæˆå™¨"""
        if not user_input.strip():
            yield "è¯·è¾“å…¥æœ‰æ•ˆçš„é—®é¢˜"
            return

        context, display_context_str = self.retrieve_context(user_input)

        if not context:
            log("âš ï¸ æœªæ£€ç´¢åˆ°ç›¸å…³å†…å®¹ï¼Œå°†ä»…åŸºäºæ¨¡å‹çŸ¥è¯†å›ç­”")
            context = "ï¼ˆçŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¸Šä¸‹æ–‡ï¼Œè¯·åŸºäºä½ è‡ªå·±çš„çŸ¥è¯†å›ç­”ï¼Œå¹¶å‘ŠçŸ¥ç”¨æˆ·çŸ¥è¯†åº“ä¸­æ— ç›¸å…³ä¿¡æ¯ï¼‰"

        system_content = (
            "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¡¬ä»¶æŠ€æœ¯åŠ©æ‰‹,ä½ çš„åå­—å«å°æ™ºã€‚è¯·ä¸¥æ ¼åŸºäºä¸‹æ–¹çš„ã€å‚è€ƒèµ„æ–™ã€‘å›ç­”ç”¨æˆ·é—®é¢˜ã€‚\n"
            "è§„åˆ™ï¼š\n"
            "1. å¦‚æœã€å‚è€ƒèµ„æ–™ã€‘åŒ…å«ç­”æ¡ˆï¼Œè¯·è¯¦ç»†å›ç­”ã€‚\n"
            "2. å¦‚æœã€å‚è€ƒèµ„æ–™ã€‘å†…å®¹ä¸è¶³æˆ–æ— å…³ï¼Œè¯·æ˜ç¡®è¯´æ˜'çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯'ï¼Œä¸è¦ç¼–é€ ã€‚\n"
            "3. å›ç­”å¿…é¡»ä½¿ç”¨ä¸­æ–‡ã€‚\n\n"
            f"### å‚è€ƒèµ„æ–™ ###\n{context}"
        )
        messages = [ChatMessage(role=MessageRole.SYSTEM, content=system_content)]
        for user_msg, bot_msg in history[-max_history:]:
            clean_bot_msg = re.split(r'\n\n---\n\n\*\*ğŸ” æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡:\*\*', bot_msg)[0]
            clean_bot_msg = re.sub(r'<[^>]+>', '', clean_bot_msg)
            messages.append(ChatMessage(role=MessageRole.USER, content=user_msg))
            messages.append(ChatMessage(role=MessageRole.ASSISTANT, content=clean_bot_msg))
        messages.append(ChatMessage(role=MessageRole.USER, content=user_input))

        try:
            response_stream = Settings.llm.stream_chat(messages)

            llm_response_content = []
            for chunk in response_stream:
                content_delta = chunk.delta or ""
                llm_response_content.append(content_delta)
                yield content_delta

            content = "".join(llm_response_content)
            log("=" * 50)
            log(f"ğŸ¤– [LLM ç”Ÿæˆè¯¦æƒ…]\n{content}")
            log("=" * 50)

            if display_context_str and "çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯" not in content:
                final_response_suffix = f"\n\n---\n\n**ğŸ” æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡:**\n{display_context_str}"
                yield final_response_suffix

        except Exception as e:
            error(f"LLMç”Ÿæˆå“åº”å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            yield "æŠ±æ­‰ï¼Œç”Ÿæˆå“åº”æ—¶å‡ºç°é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•ã€‚"
